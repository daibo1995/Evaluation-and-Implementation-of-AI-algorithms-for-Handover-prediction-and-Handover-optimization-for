{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "805f45ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import gym\n",
    "import torch\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import math\n",
    "import numpy as np\n",
    "from gym import spaces\n",
    "import pygame\n",
    "from gym.spaces import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a435c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_distance(pos1, pos2):\n",
    "    \"\"\"\n",
    "    Calculates the distance between two positions using the Euclidean distance formula.\n",
    "    \"\"\"\n",
    "    x1, y1 = pos1\n",
    "    x2, y2 = pos2\n",
    "    return math.sqrt((x2 - x1)**2 + (y2 - y1)**2)\n",
    "def calculate_signal_strength(agent, basestation):\n",
    "    base_station_params = {\n",
    "            (3, 3):(20, 2, 4),\n",
    "            (15, 3):(20, 2, 4),\n",
    "            (27, 3):(20, 2, 4),\n",
    "            (3, 15):(10, 3, 3),\n",
    "            (15, 15):(10, 3, 3),\n",
    "            (27, 15):(10, 3, 3),\n",
    "            (3, 27):(15, 1, 5),\n",
    "            (15, 27):(15, 1, 5),\n",
    "            (27, 27):(15, 1, 5),\n",
    "            (3, 39):(15, 1, 5),\n",
    "            (15, 39):(15, 1, 5),\n",
    "            (27, 39):(15, 1, 5),\n",
    "            (3, 51):(15, 1, 5),\n",
    "            (15, 51):(15, 1, 5),\n",
    "            (27, 51):(5, 1, 5),\n",
    "            \n",
    "        }\n",
    "            \n",
    "     # calculate the signal strength using this formula\n",
    "    Pt, Gt, Gr = base_station_params[basestation]\n",
    "    d = calc_distance(agent, basestation)\n",
    "    return Pt*Gt*Gr/((d+1)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "618591ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridEnvironmentdqn(gym.Env):\n",
    "    \n",
    "    def __init__(self, ranges):\n",
    "        \"\"\"\n",
    "        Initializes the environment with a grid size, base station positions, and ranges.\n",
    "        \"\"\"\n",
    "        self.grid_size = (40, 60)\n",
    "        self.base_stations=  [(3, 3),(15,3),(27,3), (3, 15),(15,15),(27,15), (3, 27),(15,27),(27,27), (3, 39),(15,39),(27,39), (3, 51), (15, 51), (27, 51)]\n",
    "        self.ranges = ranges\n",
    "        \n",
    "        self.agent_position = (20, 30)# initial position of the agent\n",
    "        self.previous_action = None # previous base station selected by the agent\n",
    "        self.new_signal_strength = 0\n",
    "        self.SINR = 0\n",
    "        self.selection_space = [bs for i, (bs, r) in enumerate(zip(self.base_stations, self.ranges)) if calc_distance(self.agent_position, bs) <= r]\n",
    "        self.new_action = random.choice(self.selection_space)\n",
    "        self.action_space = gym.spaces.Discrete(8)\n",
    "        self.handover_num = 0        \n",
    "        self.basestation_selection = random.choice(self.selection_space)\n",
    "        self.state_space = np.concatenate((np.array(self.agent_position),np.array(self.basestation_selection),np.array(self.new_signal_strength),np.array(self.SINR),np.array(self.handover_num)),axis=None)\n",
    "        self.observation_space = spaces.Box(low=0, high=60, shape=(self.state_space.shape[0],), dtype=np.float32)\n",
    "        self.reward = 0\n",
    "        self.frequency = 28e9\n",
    "        \n",
    "        self.base_station_params = {\n",
    "            (3, 3):(20, 2, 4),\n",
    "            (15, 3):(20, 2, 4),\n",
    "            (27, 3):(20, 2, 4),\n",
    "            (3, 15):(10, 3, 3),\n",
    "            (15, 15):(10, 3, 3),\n",
    "            (27, 15):(10, 3, 3),\n",
    "            (3, 27):(15, 1, 5),\n",
    "            (15, 27):(15, 1, 5),\n",
    "            (27, 27):(15, 1, 5),\n",
    "            (3, 39):(15, 1, 5),\n",
    "            (15, 39):(15, 1, 5),\n",
    "            (27, 39):(15, 1, 5),\n",
    "            (3, 51):(15, 1, 5),\n",
    "            (15, 51):(15, 1, 5),\n",
    "            (27, 51):(5, 1, 5),\n",
    "            \n",
    "        }\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment and returns the initial state.\n",
    "        \"\"\"\n",
    "        self.done = False\n",
    "#         x = random.randint(0, 39)\n",
    "#         y = random.randint(0, 59)\n",
    "        self.agent_position = (20, 30) # set the agent position to a random position on the map\n",
    "        self.previous_action = None # reset the previous action to None\n",
    "        self.basestation_selection = random.choice(self.selection_space)\n",
    "        self.new_signal_strength = self.calculate_signal_strength(self.new_action)\n",
    "        self.SINR = self.calculate_SINR(self.new_action)\n",
    "        self.handover_num = 0\n",
    "        self.reward = 0\n",
    "        self.state_space = np.concatenate((np.array(self.agent_position),np.array(self.basestation_selection),np.array(self.new_signal_strength),np.array(self.SINR),np.array(self.handover_num)),axis=None)\n",
    "        self.selection_space = [bs for i, (bs, r) in enumerate(zip(self.base_stations, self.ranges)) if calc_distance(self.agent_position, bs) <= r]\n",
    "        return self.state_space\n",
    "\n",
    "    def take_action(self, action):\n",
    "        # update the bs selection\n",
    "        \n",
    "        self.basestation_selection = action\n",
    "        # choose a random direction\n",
    "        direction = random.choice([\"up\", \"down\", \"left\", \"right\"])\n",
    "\n",
    "        # update the agent position based on the chosen direction\n",
    "        if direction == \"up\":\n",
    "            self.agent_position = (self.agent_position[0] - 1, self.agent_position[1])\n",
    "        elif direction == \"down\":\n",
    "            self.agent_position = (self.agent_position[0] + 1, self.agent_position[1])\n",
    "        elif direction == \"left\":\n",
    "            self.agent_position = (self.agent_position[0], self.agent_position[1] - 1)\n",
    "        elif direction == \"right\":\n",
    "            self.agent_position = (self.agent_position[0], self.agent_position[1] + 1)\n",
    "        # check if the agent has gone outside the map\n",
    "        \n",
    "        if self.agent_position[0] <0 or self.agent_position[0] >40 or self.agent_position[1] < 0 or self.agent_position[1] > 60:\n",
    "            self.reset()\n",
    "\n",
    "        # update the action space to only contain the base stations within range\n",
    "        self.selection_space = [bs for i, (bs, r) in enumerate(zip(self.base_stations, self.ranges)) if calc_distance(self.agent_position, bs) <= r]\n",
    "#         self.state_space = np.concatenate((np.array(self.agent_position),np.array(self.basestation_selection),np.array(self.new_signal_strength),np.array(self.SINR)),axis=None)\n",
    "                              \n",
    "    \n",
    "    def calculate_signal_strength(self, action):\n",
    "        \n",
    "         # calculate the signal strength using this formula\n",
    "        Pt, Gt, Gr = self.base_station_params[self.new_action]\n",
    "        d = calc_distance(self.agent_position, self.new_action)\n",
    "        return Pt*Gt*Gr/((d+1)**2)\n",
    "        \n",
    "    def calculate_SINR(self, action):\n",
    "        signal_power = self.new_signal_strength\n",
    "        interference_power = 0\n",
    "        noise_power = -174\n",
    "        signal_distance = calc_distance(self.agent_position, action)\n",
    "        for bs,(Pt, Gt, Gr) in self.base_station_params.items():\n",
    "            if bs != action:\n",
    "                interference_distance = calc_distance(self.agent_position, bs)\n",
    "                interference_power += Pt + Gt + Gr-(10*math.log10(interference_distance+1))\n",
    "                                        \n",
    "        self.SINR = signal_power - (interference_power + noise_power)\n",
    "        return self.SINR  \n",
    "    \n",
    "    def get_reward(self, action):\n",
    "        \n",
    "        \n",
    "        # check if there was a handover\n",
    "#         if self.previous_action is not None and self.previous_action != self.basestation_selection:\n",
    "#         self.previous_action = self.basestation_selection\n",
    "        threshold = 90\n",
    "        if self.previous_action is not None and self.previous_action != action:\n",
    "            #handover happens\n",
    "            self.reward -= 0\n",
    "            self.handover_num = 1\n",
    "            if self.new_signal_strength > self.state_space[4]:\n",
    "                self.reward += 3\n",
    "                if self.SINR > threshold:\n",
    "                    self.reward += 6\n",
    "        else: \n",
    "            self.handover_num = 0\n",
    "            self.reward += 10\n",
    "            \n",
    "            if self.new_signal_strength > self.state_space[4]:\n",
    "                \n",
    "                self.reward += 3\n",
    "                if self.SINR > threshold:\n",
    "                    self.reward += 6\n",
    "        self.basestation_selection = action\n",
    "        return self.reward\n",
    "\n",
    "    def step(self, action):\n",
    "        self.reward = 0\n",
    "        \n",
    "        if action == 0:\n",
    "            self.new_action = self.base_stations[0]\n",
    "        elif action == 1:\n",
    "            self.new_action = self.base_stations[1]\n",
    "        elif action == 2:\n",
    "            self.new_action = self.base_stations[2]\n",
    "        elif action == 3:\n",
    "            self.new_action = self.base_stations[3]\n",
    "        elif action == 4:\n",
    "            self.new_action = self.base_stations[4]\n",
    "        elif action == 5:\n",
    "            self.new_action = self.base_stations[5]\n",
    "        elif action == 6:\n",
    "            self.new_action = self.base_stations[6]\n",
    "        elif action == 7:\n",
    "            self.new_action = self.base_stations[7]\n",
    "        elif action == 8:\n",
    "            self.new_action = self.base_stations[8]\n",
    "        elif action == 9:\n",
    "            self.new_action = self.base_stations[9]\n",
    "        elif action == 10:\n",
    "            self.new_action = self.base_stations[10]\n",
    "        elif action == 11:\n",
    "            self.new_action = self.base_stations[11]\n",
    "        elif action == 12:\n",
    "            self.new_action = self.base_stations[12]\n",
    "        elif action == 13:\n",
    "            self.new_action = self.base_stations[13]\n",
    "        elif action == 14:\n",
    "            self.new_action = self.base_stations[14]\n",
    "            \n",
    "        if self.new_action not in self.selection_space:\n",
    "            self.reset()\n",
    "            self.done = True\n",
    "            self.reward -= 1000\n",
    "#             return self.state_space, self.reward, self.done, {}\n",
    "        if self.agent_position[0] <0 or self.agent_position[0] >40 or self.agent_position[1] < 0 or self.agent_position[1] > 60:\n",
    "            self.reset()\n",
    "            return self.state_space, -1,True, {}\n",
    "        self.take_action(self.new_action)\n",
    "        # take action and get the current state and done flag\n",
    "        self.new_signal_strength = self.calculate_signal_strength(self.new_action)\n",
    "        self.SINR = self.calculate_SINR(self.new_action)\n",
    "#         self.state_space = np.concatenate((np.array(self.agent_position),np.array(self.basestation_selection),np.array(self.new_signal_strength),np.array(self.SINR)),axis=None)\n",
    "        # get the reward\n",
    "        \n",
    "        reward = self.get_reward(self.new_action)\n",
    "        \n",
    "        # update previous action\n",
    "        self.previous_action = self.basestation_selection\n",
    "        # update the signal strength\n",
    "        \n",
    "        self.state_space = np.concatenate((np.array(self.agent_position),np.array(self.basestation_selection),np.array(self.new_signal_strength),np.array(self.SINR),np.array(self.handover_num)),axis=None)\n",
    "        observation = np.concatenate((np.array(self.agent_position),np.array(self.basestation_selection),np.array(self.new_signal_strength),np.array(self.SINR),np.array(self.handover_num)),axis=None)\n",
    "        \n",
    "        # render the environment\n",
    "        self.render()\n",
    "\n",
    "        # return the state, reward, done flag, and an empty dictionary\n",
    "        return observation, reward, self.done, {}\n",
    "\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # initialize Pygame\n",
    "        pygame.init()\n",
    "        # set the window size\n",
    "        screen = pygame.display.set_mode((400, 600))\n",
    "        # set the window title\n",
    "        pygame.display.set_caption(\"Grid Environment\")\n",
    "        # set the background color to white\n",
    "        screen.fill((255, 255, 255))\n",
    "        # draw the grid\n",
    "        for i in range(41):\n",
    "            pygame.draw.line(screen, (0, 0, 0), (i*10, 0), (i*10, 600))\n",
    "        for i in range(61):\n",
    "            pygame.draw.line(screen, (0, 0, 0), (0, i*10), (400, i*10))\n",
    "        # draw the base stations\n",
    "        for bs in self.base_stations:\n",
    "            pygame.draw.circle(screen, (0, 0, 255), (bs[0]*10+5, bs[1]*10+5), 5)\n",
    "        \n",
    "        # draw the agent\n",
    "        pygame.draw.circle(screen, (255, 0, 0), (self.agent_position[0]*10+5, self.agent_position[1]*10+5), 5)\n",
    "        # display the action space\n",
    "        font = pygame.font.Font(None, 36)\n",
    "        text = font.render(f\"Action Space: {self.selection_space}\", True, (0, 0, 0))\n",
    "        screen.blit(text, (10, 10))\n",
    "        # display the agent position\n",
    "        text = font.render(f\"Agent Position: {self.agent_position}\", True, (0, 0, 0))\n",
    "        screen.blit(text, (10, 50))\n",
    "        # display the accumulated reward\n",
    "        text = font.render(f\"Reward: {self.reward}\", True, (0, 0, 0))\n",
    "        screen.blit(text, (10, 90))\n",
    "        # update the screen\n",
    "        if mode == 'human':\n",
    "          # Update the display\n",
    "          pygame.display.flip()\n",
    "        elif mode == 'rgb_array':\n",
    "          # Return the rendered image as a NumPy array\n",
    "          return np.array(pygame.surfarray.array3d(screen))\n",
    "\n",
    "\n",
    "        \n",
    "    def close(self):\n",
    "        # Close the pygame window\n",
    "        pygame.display.quit()\n",
    "\n",
    "        # Shut down pygame\n",
    "        pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfcd42fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import gym\n",
    "import torch\n",
    "import random\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "env = GridEnvironmentdqn([15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15,15])\n",
    "env = DummyVecEnv([lambda: env])\n",
    "log_path = os.path.join(\"Training2\", \"Logs\")\n",
    "import tensorflow as tf\n",
    "\n",
    "writer = tf.summary.create_file_writer(log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c6c083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN_path = os.path.join('Training', 'Saved Models', 'DQN_Model_300000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b3734a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "model = DQN('MlpPolicy', env, verbose=1, tensorboard_log = log_path,batch_size=2048,learning_starts=25000,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "a9ffdd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "DQN_path = os.path.join('Training', 'Saved Models', 'DQN_Model_300000with2048')\n",
    "model = DQN.load(DQN_path, env= env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2c5229",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "05b91663",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(DQN_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10ab0d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7484ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fe0d182",
   "metadata": {},
   "source": [
    "# PPO training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "b2fe8af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_path = os.path.join('Training', 'Saved Models', 'PPO_Model_300000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "8b6b4e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "model = PPO('MlpPolicy', env, verbose=1, tensorboard_log = log_path,n_steps = 2048, learning_rate=0.0003, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2ae39fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO.load(PPO_path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fb9379",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "6d9aba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PPO_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fea261f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a5408fa",
   "metadata": {},
   "source": [
    "# a2c training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f759cc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "A2C_path = os.path.join('Training', 'Saved Models', 'A2C_Model_300000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7ae3bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "model = A2C(\"MlpPolicy\", env, verbose=1,tensorboard_log = log_path,n_steps= 2048, learning_rate = 0.0005, gamma=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074aab6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C.load(A2C_path,env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "35a2dfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training2\\Logs\\A2C_13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.a2c.a2c.A2C at 0x247fa3f4d30>"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "7b0bc2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(A2C_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e998896e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
